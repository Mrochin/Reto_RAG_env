{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "m# RETO RAG — Implementación End-to-End (Paso 3)\n",
    "\n",
    "Este notebook implementa un **sistema RAG (Retrieval-Augmented Generation)** en entorno local (Anaconda + PyCharm).\n",
    "Incluye: ingestión de PDF, chunking, embeddings, base vectorial **ChromaDB**, y pipeline de Recuperación + Generación.\n",
    "\n",
    "**Requisitos mínimos**: `pip install --no-cache-dir langchain langchain-openai chromadb pypdf`\n",
    "(Opcionales: `tiktoken`, `papermill`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0) Configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#cargar variables desde .env\n",
    "load_dotenv()\n",
    "print(\"Api Key presente:\", \"OPENAI_API_KEY\" in os.environ,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PDFS_DIR = Path(\"docs\")\n",
    "PDF_FILE = PDFS_DIR / \"ejemplo.pdf\"\n",
    "CHROMA_PERSIST_DIR = Path(\"./.chroma_db_local\")\n",
    "\n",
    "OPENAI_EMBED_MODEL = \"text-embedding-3-small\"\n",
    "OPENAI_CHAT_MODEL  = \"gpt-4o-mini\"\n",
    "\n",
    "PDFS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CHROMA_PERSIST_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"OPENAI_API_KEY presente:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1) Ingesta de PDF → Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    return \"\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "\n",
    "if PDF_FILE.exists():\n",
    "    raw_text = extract_text_from_pdf(PDF_FILE)\n",
    "    print(\"Longitud del texto extraído:\", len(raw_text))\n",
    "else:\n",
    "    raw_text = \"\"\n",
    "    print(f\"Coloca un PDF en {PDF_FILE} y vuelve a ejecutar esta celda.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2) Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(text: str, chunk_size=500, chunk_overlap=80):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "chunks = chunk_text(raw_text) if raw_text else []\n",
    "print(f\"Total de fragmentos: {len(chunks)}\")\n",
    "if chunks[:1]:\n",
    "    print(\"Ejemplo de fragmento:\\n\", chunks[0][:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3) Embeddings + ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def build_vector_store(text_chunks, persist_dir: Path):\n",
    "    if not text_chunks:\n",
    "        raise ValueError(\"No hay fragmentos para indexar. Verifica el PDF y el chunking.\")\n",
    "    emb = OpenAIEmbeddings(model=OPENAI_EMBED_MODEL)\n",
    "    # Al crear con persist_directory, Chroma persiste automáticamente (>=0.4)\n",
    "    db = Chroma.from_texts(text_chunks, emb, persist_directory=str(persist_dir))\n",
    "    return db\n",
    "\n",
    "db = None\n",
    "if chunks:\n",
    "    db = build_vector_store(chunks, CHROMA_PERSIST_DIR)\n",
    "    print(\"ChromaDB inicializada en:\", CHROMA_PERSIST_DIR)\n",
    "else:\n",
    "    print(\"No se construyó la base vectorial (sin fragmentos).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4) Recuperación + Generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def build_rag_chain(chroma_db, chat_model=OPENAI_CHAT_MODEL, k=4):\n",
    "    llm = ChatOpenAI(model=chat_model)\n",
    "    retriever = chroma_db.as_retriever(search_kwargs={\"k\": k})\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, retriever=retriever, return_source_documents=True\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "qa = build_rag_chain(db) if db else None\n",
    "if qa: print(\"Pipeline RAG listo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5) Prueba rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"¿Cuáles son los conceptos clave mencionados en el documento?\"\n",
    "if qa and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    result = qa.invoke({\"query\": query})\n",
    "    print(\"Respuesta:\", result[\"result\"])\n",
    "    print(\"\\n---\\nFragmentos usados:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], start=1):\n",
    "        print(f\"[{i}] {doc.page_content[:200]}...\")\n",
    "elif qa and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Configura OPENAI_API_KEY antes de consultar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"¿De que trata el concepto hsbt?\"\n",
    "if qa and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    result = qa.invoke({\"query\": query})\n",
    "    print(\"Respuesta:\", result[\"result\"])\n",
    "    print(\"\\n---\\nFragmentos usados:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], start=1):\n",
    "        print(f\"[{i}] {doc.page_content[:200]}...\")\n",
    "elif qa and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Configura OPENAI_API_KEY antes de consultar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"¿Regla para el cálculo de Hsbt?\"\n",
    "if qa and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    result = qa.invoke({\"query\": query})\n",
    "    print(\"Respuesta:\", result[\"result\"])\n",
    "    print(\"\\n---\\nFragmentos usados:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"], start=1):\n",
    "        print(f\"[{i}] {doc.page_content[:200]}...\")\n",
    "elif qa and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Configura OPENAI_API_KEY antes de consultar.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
