{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pr√°ctica 5 Automatizaci√≥n y Optimizaci√≥n Avanzada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "> Objetivo central: **\"Automatiza un stack CRUD completo (modelo, API, tests e infraestructura) orquestando LLMs con LangChain de forma reproducible y medible.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ¬øQu√© voy a lograr y por qu√© importa?\n",
    "En esta pr√°ctica construyes un **pipeline automatizado** que, partiendo de una configuraci√≥n declarativa (YAML / Pydantic), genera:\n",
    "- Modelos Pydantic validados\n",
    "- Router FastAPI empresarial (CRUD + paginaci√≥n + auth opcional)\n",
    "- Suite de tests Pytest\n",
    "- Infraestructura (Dockerfile, migraci√≥n Alembic)\n",
    "- M√©tricas de eficiencia de la generaci√≥n\n",
    "\n",
    "Esto refleja un caso real: equipos que necesitan **acelerar el scaffolding backend** manteniendo est√°ndares de calidad. Aprender√°s a usar **LangChain Expression Language (LCEL)** y `RunnableParallel` para ejecutar tareas en paralelo y encadenar dependencias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Problemas reales que esto resuelve:\n",
    "- Onboarding lento: crear cada CRUD manualmente tarda horas.\n",
    "- Inconsistencia entre servicios: estilos diferentes de validaci√≥n / logs.\n",
    "- Falta de medici√≥n: se generan cosas con IA pero sin m√©tricas.\n",
    "- Riesgo t√©cnico: prompts desordenados, sin control de dependencias.\n",
    "\n",
    "Soluci√≥n mostrada: un **pipeline determinista** donde cada bloque tiene una responsabilidad clara. As√≠ escalas generaci√≥n de servicios sin sacrificar mantenibilidad.\n",
    "\n",
    "Rol de las piezas:\n",
    "- `ResourceConfig` y `FieldConfig`: contrato declarativo de tu recurso.\n",
    "- Prompts especializados (model, router, tests, infra): separaci√≥n de dominios (Domain Prompting).\n",
    "- `RunnableParallel`: acelera la generaci√≥n base (modelo + config) y luego deriva dependientes.\n",
    "- `Structured Output`: fuerza esquemas (`InfrastructureComponents`).\n",
    "- M√©tricas: base para gobernanza y ROI de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "| Concepto | Idea-faro | Analog√≠a |\n",
    "|----------|-----------|----------|\n",
    "| LCEL | Orquesta modular | \"LEGO de flujos LLM\" |\n",
    "| `RunnableParallel` | Paralelismo declarativo | \"Carriles simult√°neos\" |\n",
    "| Config ‚Üí Artefactos | Infra como c√≥digo pero para scaffolding | \"Terraform de tu backend\" |\n",
    "| Prompts especializados | Principio de responsabilidad √∫nica | \"Microservicios cognitivos\" |\n",
    "| M√©tricas | Observabilidad del pipeline | \"Tablero de control DevOps\" |\n",
    "| Structured Output | Control sint√°ctico | \"Molde para la arcilla del modelo\" |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Pr√°ctica paso a paso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Parte 1: Setup del Entorno\n",
    "\n",
    "Configuraremos un entorno completo con herramientas de an√°lisis y generaci√≥n automatizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai langchain-community fastapi uvicorn pydantic pytest httpx python-dotenv jinja2 pyyaml click rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Literal\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  Configura OPENAI_API_KEY en tu archivo .env\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API Key configurada\")\n",
    "\n",
    "# Configurar modelo\n",
    "model = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "print(\"ü§ñ Modelo listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 4.2 Modelos de Configuraci√≥n\n",
    "Implementaremos un sistema avanzado de generaci√≥n CRUD usando RunnableParallel y configuraci√≥n YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Union\n",
    "\n",
    "# Modelos de configuraci√≥n\n",
    "\n",
    "class FieldConfig(BaseModel):\n",
    "    name: str\n",
    "    type: Literal[\"int\", \"float\", \"str\", \"bool\"]\n",
    "    description: str = \"\"\n",
    "    constraints: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class ResourceConfig(BaseModel):\n",
    "    resource_name: str\n",
    "    class_name: str\n",
    "    fields: List[FieldConfig]\n",
    "    auth_required: bool = False\n",
    "    cache_enabled: bool = False\n",
    "    pagination: bool = True\n",
    "    soft_delete: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear configuraci√≥n de ejemplo\n",
    "sample_config = ResourceConfig(\n",
    "    resource_name=\"product\",\n",
    "    class_name=\"Product\", \n",
    "    fields=[\n",
    "        FieldConfig(name=\"name\", type=\"str\", description=\"Nombre del producto\", \n",
    "                   constraints={\"min_length\": 1, \"max_length\": 100}),\n",
    "        FieldConfig(name=\"price\", type=\"float\", description=\"Precio en USD\",\n",
    "                   constraints={\"ge\": 0, \"le\": 999999}),\n",
    "        FieldConfig(name=\"category\", type=\"str\", required=False, description=\"Categor√≠a\"),\n",
    "        FieldConfig(name=\"stock\", type=\"int\", description=\"Cantidad en inventario\",\n",
    "                   constraints={\"ge\": 0})\n",
    "    ],\n",
    "    auth_required=True,\n",
    "    cache_enabled=True,\n",
    "    pagination=True,\n",
    "    soft_delete=True\n",
    ")\n",
    "\n",
    "# Guardar configuraci√≥n para uso posterior\n",
    "config_path = Path(\"resource_configs\") \n",
    "config_path.mkdir(exist_ok=True)\n",
    "with open(config_path / \"product_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(sample_config.dict(), f, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Parte 3: Generadores Especializados con RunnableParallel\n",
    "\n",
    "Crearemos generadores especializados que trabajen en paralelo para m√°xima eficiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Se crean 4 generadores:\n",
    "1. `model_generator`: produce modelos Pydantic (entrada, salida, update, validaciones y validators).\n",
    "2. `router_generator`: crea router FastAPI con CRUD completo y middlewares condicionales.\n",
    "3. `tests_generator`: dise√±a suite Pytest (unit, integration, performance b√°sico, edge cases).\n",
    "4. `infra_generator`: con `with_structured_output` para garantizar campos (`dockerfile`, `migration`, etc.).\n",
    "\n",
    "Dise√±o de prompts: cada uno declara expl√≠citamente criterios de calidad (ej. \"nivel PRODUCCI√ìN\", \"validaciones espec√≠ficas\", \"logging estructurado\"). Esto reduce alucinaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# GENERADORES ESPECIALIZADOS \n",
    "\n",
    "class GeneratedComponents(BaseModel):\n",
    "    modelo_pydantic: str = Field(description=\"Modelo Pydantic con validaciones avanzadas\")\n",
    "    router_fastapi: str = Field(description=\"Router FastAPI con CRUD completo\")\n",
    "    tests_pytest: str = Field(description=\"Suite de tests exhaustiva\")\n",
    "    alembic_migration: str = Field(description=\"Migraci√≥n Alembic para base de datos\")\n",
    "    dockerfile: str = Field(description=\"Dockerfile optimizado\")\n",
    "\n",
    "\n",
    "# Templates\n",
    "model_generator =(ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Eres experto en Pydantic y dise√±o de APIs con FastAPI.\n",
    "        Genera modelos Pydantic de nivel PRODUCCI√ìN con:\n",
    "        - Validaciones espec√≠ficas por tipo de campo\n",
    "        - Docstrings detallados con ejemplos\n",
    "        - Field constraints apropiados\n",
    "        - Validators personalizados para l√≥gica de negocio\n",
    "        - Modelos de entrada, salida y actualizaci√≥n separados\n",
    "        \"\"\"),\n",
    "                (\"human\", \"\"\"\n",
    "        CONFIGURACI√ìN:\n",
    "        Resource: {resource_name}\n",
    "        Class: {class_name}\n",
    "        Fields: {fields}\n",
    "        Features: auth={auth_required}, cache={cache_enabled}, soft_delete={soft_delete}\n",
    "\n",
    "        Genera modelos Pydantic profesionales con validaciones robustas.\n",
    "        \"\"\")\n",
    "    ])\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generador de routers FastAPI completos\n",
    "router_generator = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Eres arquitecto senior FastAPI especialista en APIs RESTful.\n",
    "        Genera routers de PRODUCCI√ìN con:\n",
    "        - Endpoints CRUD completos (GET, POST, PUT, PATCH, DELETE)\n",
    "        - Paginaci√≥n, filtrado y ordenamiento\n",
    "        - Manejo de errores HTTP consistente\n",
    "        - Documentaci√≥n OpenAPI rica\n",
    "        - Middlewares de auth y cache seg√∫n configuraci√≥n\n",
    "        - Logging estructurado\n",
    "        - Validaci√≥n de permisos\n",
    "        \"\"\"),\n",
    "                (\"human\", \"\"\"\n",
    "        MODELO PYDANTIC:\n",
    "        {modelo_pydantic}\n",
    "\n",
    "        CONFIGURACI√ìN:\n",
    "        {config}\n",
    "\n",
    "        Genera router FastAPI de nivel empresarial.\n",
    "    \"\"\")\n",
    "    ])\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generador de tests exhaustivos\n",
    "tests_generator = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Eres QA Lead especialista en testing de APIs.\n",
    "        Genera suite de tests COMPREHENSIVA con:\n",
    "        - Tests unitarios para cada endpoint\n",
    "        - Tests de integraci√≥n end-to-end\n",
    "        - Tests de performance b√°sicos\n",
    "        - Tests de seguridad (auth, validation)\n",
    "        - Tests de casos borde y error handling\n",
    "        - Fixtures y mocks apropiados\n",
    "        - Cobertura de al menos 90%\n",
    "        \"\"\"),\n",
    "                (\"human\", \"\"\"\n",
    "        ROUTER FASTAPI:\n",
    "        {router_fastapi}\n",
    "\n",
    "        MODELO PYDANTIC:\n",
    "        {modelo_pydantic}\n",
    "\n",
    "        CONFIGURACI√ìN:\n",
    "        {config}\n",
    "\n",
    "        Genera tests pytest de nivel empresarial.\n",
    "        \"\"\")\n",
    "    ])\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Modelo para infraestructura\n",
    "class InfrastructureComponents(BaseModel):\n",
    "    dockerfile: str = Field(description=\"Dockerfile multi-stage optimizado\")\n",
    "    migration: str = Field(description=\"Migraci√≥n Alembic con √≠ndices\")\n",
    "    docker_compose: str = Field(description=\"Docker-compose para desarrollo\", default=\"\")\n",
    "    deployment_script: str = Field(description=\"Script de deployment\", default=\"\")\n",
    "\n",
    "# 4. Generador de infraestructura (Docker, migrations) - corregido\n",
    "infra_generator = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres DevOps senior especialista en containerizaci√≥n y databases.\n",
    "    Genera infraestructura de PRODUCCI√ìN:\n",
    "    - Dockerfile multi-stage optimizado\n",
    "    - Migraci√≥n Alembic con √≠ndices apropiados\n",
    "    - Docker-compose para desarrollo\n",
    "    - Scripts de deployment\n",
    "    \"\"\"),\n",
    "        (\"human\", \"\"\"\n",
    "    CONFIGURACI√ìN:\n",
    "    {config}\n",
    "\n",
    "    MODELO PYDANTIC:\n",
    "    {modelo_pydantic}\n",
    "\n",
    "    Genera infraestructura completa para producci√≥n.\n",
    "    \"\"\")\n",
    "]) | model.with_structured_output(InfrastructureComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 4.4 Orquestaci√≥n LCEL\n",
    "Funci√≥n `create_advanced_crud_pipeline()`:\n",
    "- Fase base paralela: genera `modelo` y pasa `config` intacta.\n",
    "- Lambda intermedia `_generate_dependent_components`: usa salida anterior para generar router y luego en paralelo tests + infraestructura.\n",
    "- Se empaqueta todo en un `GeneratedComponents` final.\n",
    "\n",
    "Ventaja: minimiza latencia (paraleliza lo que no depende) y mantiene orden l√≥gico de dependencias (modelo ‚Üí router ‚Üí tests/infra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_dependent_components(base_components):\n",
    "    config = base_components[\"config\"]\n",
    "    modelo = base_components[\"modelo\"]\n",
    "\n",
    "    router = router_generator.invoke({\n",
    "        \"modelo_pydantic\": modelo,\n",
    "        \"config\": json.dumps(config, indent=2, ensure_ascii=False)\n",
    "    })\n",
    "\n",
    "    parallel_final = RunnableParallel({\n",
    "        \"tests\": tests_generator,\n",
    "        \"infra\": infra_generator\n",
    "    })\n",
    "\n",
    "    final_components = parallel_final.invoke({\n",
    "        \"router_fastapi\": router,\n",
    "        \"modelo_pydantic\": modelo,\n",
    "        \"config\": json.dumps(config, indent=2, ensure_ascii=False)\n",
    "    })\n",
    "\n",
    "    infra = final_components[\"infra\"]\n",
    "    return GeneratedComponents(\n",
    "        modelo_pydantic=modelo,\n",
    "        router_fastapi=router,\n",
    "        tests_pytest=final_components[\"tests\"],\n",
    "        alembic_migration=infra.migration if infra else \"\",\n",
    "        dockerfile=infra.dockerfile if infra else \"\"\n",
    "    )\n",
    "\n",
    "def create_advanced_crud_pipeline():\n",
    "    parallel_base = RunnableParallel({\n",
    "        \"modelo\": model_generator,\n",
    "        \"config\": RunnableLambda(lambda x: x)\n",
    "    })\n",
    "    return parallel_base | RunnableLambda(_generate_dependent_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "crud_pipeline = create_advanced_crud_pipeline()\n",
    "\n",
    "crud_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 4.5 Ejecuci√≥n + M√©tricas\n",
    "Se prepara input a partir de `sample_config` y se invoca `crud_pipeline.invoke(pipeline_input)`.\n",
    "\n",
    "M√©tricas recolectadas manualmente (se esboza callback pero no se conecta en la ejecuci√≥n actual):\n",
    "- Tiempo total de generaci√≥n\n",
    "- Conteo de l√≠neas por componente\n",
    "- N√∫mero de clases, endpoints, validators, fixtures, asserts\n",
    "- C√°lculo de ROI: (tiempo manual estimado / tiempo IA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUCI√ìN DEL PIPELINE CON M√âTRICAS AVANZADAS\n",
    "\n",
    "# 1. Callback para capturar m√©tricas detalladas\n",
    "class CRUDGenerationMetrics(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"components_generated\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"llm_calls\": 0,\n",
    "            \"parallel_executions\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"component_times\": {}\n",
    "        }\n",
    "        self.current_component = None\n",
    "        self.component_start = None\n",
    "    \n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        if not self.metrics[\"start_time\"]:\n",
    "            self.metrics[\"start_time\"] = time.time()\n",
    "        self.component_start = time.time()\n",
    "    \n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        if self.component_start:\n",
    "            duration = time.time() - self.component_start\n",
    "            component_name = self.current_component or \"unknown\"\n",
    "            self.metrics[\"component_times\"][component_name] = duration\n",
    "            self.metrics[\"components_generated\"] += 1\n",
    "        \n",
    "        self.metrics[\"end_time\"] = time.time()\n",
    "    \n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        self.metrics[\"llm_calls\"] += 1\n",
    "    \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        if hasattr(response, 'llm_output') and response.llm_output:\n",
    "            token_usage = response.llm_output.get('token_usage', {})\n",
    "            self.metrics[\"total_tokens\"] += token_usage.get('total_tokens', 0)\n",
    "    \n",
    "    def on_chain_error(self, error, **kwargs):\n",
    "        self.metrics[\"errors\"] += 1\n",
    "    \n",
    "    def get_summary(self):\n",
    "        total_time = (self.metrics[\"end_time\"] or time.time()) - (self.metrics[\"start_time\"] or time.time())\n",
    "        return {\n",
    "            **self.metrics,\n",
    "            \"total_time\": total_time,\n",
    "            \"avg_time_per_component\": total_time / max(self.metrics[\"components_generated\"], 1),\n",
    "            \"tokens_per_second\": self.metrics[\"total_tokens\"] / max(total_time, 1),\n",
    "            \"success_rate\": (self.metrics[\"components_generated\"] - self.metrics[\"errors\"]) / max(self.metrics[\"components_generated\"], 1) * 100\n",
    "        }\n",
    "\n",
    "# 2. Ejecutar generaci√≥n con m√©tricas\n",
    "print(\"üöÄ INICIANDO GENERACI√ìN CRUD AUTOMATIZADA...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics_callback = CRUDGenerationMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_config.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar input para el pipeline\n",
    "pipeline_input = {\n",
    "    \"resource_name\": sample_config.resource_name,\n",
    "    \"class_name\": sample_config.class_name,\n",
    "    \"fields\": [f.dict() for f in sample_config.fields],\n",
    "    \"auth_required\": sample_config.auth_required,\n",
    "    \"cache_enabled\": sample_config.cache_enabled,\n",
    "    \"soft_delete\": sample_config.soft_delete\n",
    "}\n",
    "\n",
    "pipeline_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el pipeline\n",
    "start_generation = time.time()\n",
    "generated_components = crud_pipeline.invoke(pipeline_input)\n",
    "generation_time = time.time() - start_generation\n",
    "\n",
    "print(f\"Generaci√≥n Completa en {generation_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. An√°lisis de los componentes generados\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä AN√ÅLISIS DE COMPONENTES GENERADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "components_analysis = {\n",
    "    \"modelo\": {\n",
    "        \"lines\": generated_components.modelo_pydantic.count('\\n'),\n",
    "        \"classes\": generated_components.modelo_pydantic.count('class '),\n",
    "        \"validations\": generated_components.modelo_pydantic.count('Field('),\n",
    "        \"validators\": generated_components.modelo_pydantic.count('@validator')\n",
    "    },\n",
    "    \"router\": {\n",
    "        \"lines\": generated_components.router_fastapi.count('\\n'),\n",
    "        \"endpoints\": len([x for x in ['@app.get', '@app.post', '@app.put', '@app.delete'] \n",
    "                        if x in generated_components.router_fastapi]),\n",
    "        \"error_handling\": generated_components.router_fastapi.count('HTTPException'),\n",
    "        \"documentation\": generated_components.router_fastapi.count('summary=')\n",
    "    },\n",
    "    \"tests\": {\n",
    "        \"lines\": generated_components.tests_pytest.count('\\n'),\n",
    "        \"test_functions\": generated_components.tests_pytest.count('def test_'),\n",
    "        \"assertions\": generated_components.tests_pytest.count('assert '),\n",
    "        \"fixtures\": generated_components.tests_pytest.count('@pytest.fixture')\n",
    "    }\n",
    "}\n",
    "\n",
    "for component, analysis in components_analysis.items():\n",
    "    print(f\"\\nüîç {component.upper()}:\")\n",
    "    for metric, value in analysis.items():\n",
    "        print(f\"  ‚Ä¢ {metric.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_components.router_fastapi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 4.6 Persistencia de Artefactos\n",
    "Escribe a carpeta `generated_product_api/` solo archivos con contenido.\n",
    "- `models.py`\n",
    "- `routes.py`\n",
    "- `test_api.py`\n",
    "- `Dockerfile`\n",
    "- `migration.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Guardar componentes generados\n",
    "output_dir = Path(f\"generated_{sample_config.resource_name}_api\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "files_created = {\n",
    "    \"models.py\": generated_components.modelo_pydantic,\n",
    "    \"routes.py\": generated_components.router_fastapi,\n",
    "    \"test_api.py\": generated_components.tests_pytest,\n",
    "    \"Dockerfile\": generated_components.dockerfile,\n",
    "    \"migration.py\": generated_components.alembic_migration\n",
    "}\n",
    "\n",
    "for filename, content in files_created.items():\n",
    "    if content.strip():  # Solo crear archivos con contenido\n",
    "        file_path = output_dir / filename\n",
    "        file_path.write_text(content)\n",
    "\n",
    "print(f\"\\nüíæ Archivos generados en: {output_dir}\")\n",
    "print(f\"üìÅ Total archivos: {len([f for f in files_created.values() if f.strip()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Si ven√≠as de:\n",
    "- Prompt Engineering b√°sico ‚Üí ahora formalizas prompts como componentes reutilizables.\n",
    "- FastAPI manual ‚Üí ahora generas scaffolding consistente.\n",
    "- DevOps / Infra ‚Üí introduces IaC-like patterns para artefactos de backend.\n",
    "\n",
    "Idea principal para recordar: **\"Estandariza y paraleliza lo generable; reserva tu tiempo humano para lo verdaderamente diferencial.\"**\n",
    "\n",
    "Checklist mental (READY):\n",
    "- ¬øTengo config declarativa? ‚úÖ\n",
    "- ¬øPrompts con criterios expl√≠citos? ‚úÖ\n",
    "- ¬øControl de dependencias y orden? ‚úÖ\n",
    "- ¬øM√©tricas de eficiencia? ‚úÖ\n",
    "- ¬øArtefactos persistidos y reutilizables? ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Si ven√≠as de:\n",
    "- Prompt Engineering b√°sico ‚Üí ahora formalizas prompts como componentes reutilizables.\n",
    "- FastAPI manual ‚Üí ahora generas scaffolding consistente.\n",
    "- DevOps / Infra ‚Üí introduces IaC-like patterns para artefactos de backend.\n",
    "\n",
    "Idea principal para recordar: **\"Estandariza y paraleliza lo generable; reserva tu tiempo humano para lo verdaderamente diferencial.\"**\n",
    "\n",
    "Checklist mental (READY):\n",
    "- ¬øTengo config declarativa? ‚úÖ\n",
    "- ¬øPrompts con criterios expl√≠citos? ‚úÖ\n",
    "- ¬øControl de dependencias y orden? ‚úÖ\n",
    "- ¬øM√©tricas de eficiencia? ‚úÖ\n",
    "- ¬øArtefactos persistidos y reutilizables? ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Parte 1: Pipeline de Refactoring Inteligente\n",
    "\n",
    "Implementaremos un sistema que analiza c√≥digo existente y propone mejoras autom√°ticas basadas en m√©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# PIPELINE DE REFACTORING INTELIGENTE\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefactoringAnalysis(BaseModel):\n",
    "    \"\"\"An√°lisis integral para refactoring\"\"\"\n",
    "    code_issues: List[str] = Field(description=\"Lista de problemas detectados en el c√≥digo\")\n",
    "    refactoring_priority: str = Field(description=\"Alta/Media/Baja prioridad de refactoring\")\n",
    "    improvement_suggestions: List[str] = Field(description=\"Sugerencias espec√≠ficas de mejora\")\n",
    "    estimated_effort: str = Field(description=\"Estimaci√≥n de esfuerzo: Bajo/Medio/Alto\")\n",
    "    maintainability_score: int = Field(description=\"Puntuaci√≥n de mantenibilidad (1-10)\")\n",
    "\n",
    "class RefactoredCode(BaseModel):\n",
    "    \"\"\"C√≥digo refactorizado con mejoras\"\"\"\n",
    "    improved_code: str = Field(description=\"C√≥digo mejorado y refactorizado\")\n",
    "    changes_summary: List[str] = Field(description=\"Resumen de cambios realizados\")\n",
    "    performance_improvements: List[str] = Field(description=\"Mejoras de rendimiento aplicadas\")\n",
    "    code_style_fixes: List[str] = Field(description=\"Correcciones de estilo de c√≥digo\")\n",
    "\n",
    "# Parsers\n",
    "analysis_parser = PydanticOutputParser(pydantic_object=RefactoringAnalysis)\n",
    "refactoring_parser = PydanticOutputParser(pydantic_object=RefactoredCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de an√°lisis y refactoring\n",
    "def create_refactoring_pipeline():\n",
    "    \"\"\"Crear pipeline inteligente de refactoring\"\"\"\n",
    "    \n",
    "    # An√°lisis t√©cnico\n",
    "    technical_analysis = RunnableLambda(\n",
    "        lambda x: f\"\"\"\n",
    "        Analiza el siguiente c√≥digo y proporciona un an√°lisis t√©cnico detallado:\n",
    "\n",
    "        C√ìDIGO:\n",
    "        {x['code']}\n",
    "\n",
    "        M√âTRICAS DETECTADAS:\n",
    "        {x['metrics']}\n",
    "\n",
    "        PATRONES DETECTADOS:\n",
    "        {x['patterns']}\n",
    "\n",
    "        Proporciona un an√°lisis integral considerando:\n",
    "        1. Problemas de complejidad y mantenibilidad\n",
    "        2. Anti-patrones detectados\n",
    "        3. Oportunidades de mejora\n",
    "        4. Priorizaci√≥n de refactoring\n",
    "\n",
    "        {analysis_parser.get_format_instructions()}\n",
    "        \"\"\"\n",
    "    ) | model | analysis_parser\n",
    "\n",
    "    # Refactoring inteligente\n",
    "    intelligent_refactoring = RunnableLambda(\n",
    "        lambda x: f\"\"\"\n",
    "        Refactoriza el siguiente c√≥digo bas√°ndote en el an√°lisis t√©cnico:\n",
    "\n",
    "        C√ìDIGO ORIGINAL:\n",
    "        {x['code']}\n",
    "\n",
    "        AN√ÅLISIS T√âCNICO:\n",
    "        {x['analysis']}\n",
    "\n",
    "        DIRECTRICES DE REFACTORING:\n",
    "        1. Reducir complejidad ciclom√°tica manteniendo funcionalidad\n",
    "        2. Aplicar principios SOLID y clean code\n",
    "        3. Mejorar legibilidad y mantenibilidad\n",
    "        4. Optimizar rendimiento donde sea posible\n",
    "        5. Mantener compatibilidad con APIs existentes\n",
    "        6. Agregar documentaci√≥n y tipado donde falte\n",
    "\n",
    "        IMPORTANTE: El c√≥digo debe seguir mejores pr√°cticas de FastAPI y Pydantic.\n",
    "\n",
    "        {refactoring_parser.get_format_instructions()}\n",
    "        \"\"\"\n",
    "    ) | model | refactoring_parser\n",
    "\n",
    "    # Pipeline paralelo\n",
    "    refactoring_pipeline = RunnableParallel({\n",
    "        \"analysis\": technical_analysis,\n",
    "        \"code\": RunnableLambda(lambda x: x[\"code\"]),\n",
    "        \"metrics\": RunnableLambda(lambda x: x[\"metrics\"]),\n",
    "        \"patterns\": RunnableLambda(lambda x: x[\"patterns\"])\n",
    "    }) | RunnableLambda(\n",
    "        lambda x: {\n",
    "            \"code\": x[\"code\"],\n",
    "            \"analysis\": x[\"analysis\"],\n",
    "            \"metrics\": x[\"metrics\"],\n",
    "            \"patterns\": x[\"patterns\"]\n",
    "        }\n",
    "    ) | RunnableParallel({\n",
    "        \"original\": RunnableLambda(lambda x: x),\n",
    "        \"refactored\": intelligent_refactoring\n",
    "    })\n",
    "\n",
    "    return refactoring_pipeline\n",
    "\n",
    "# Crear pipeline\n",
    "refactoring_system = create_refactoring_pipeline()\n",
    "\n",
    "print(\"üöÄ Pipeline de refactoring inteligente creado\")\n",
    "print(\"  ‚Ä¢ An√°lisis t√©cnico autom√°tico\")\n",
    "print(\"  ‚Ä¢ Refactoring basado en m√©tricas\")\n",
    "print(\"  ‚Ä¢ Optimizaci√≥n de c√≥digo paralela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactoring_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMOSTRACI√ìN: Refactoring inteligente de c√≥digo legacy\n",
    "import time\n",
    "\n",
    "# C√≥digo de ejemplo con problemas detectables\n",
    "sample_legacy_code = '''\n",
    "def process_user_data(data):\n",
    "    print(\"Processing user data...\")\n",
    "    result = []\n",
    "    \n",
    "    # TODO: Add input validation\n",
    "    for item in data:\n",
    "        print(f\"Processing item: {item}\")\n",
    "        \n",
    "        if item[\"age\"] > 18 and item[\"status\"] == \"active\":\n",
    "            processed = {}\n",
    "            processed[\"id\"] = item[\"id\"]\n",
    "            processed[\"name\"] = item[\"name\"] \n",
    "            processed[\"email\"] = item[\"email\"]\n",
    "            processed[\"age\"] = item[\"age\"]\n",
    "            processed[\"processed_at\"] = time.time()\n",
    "            \n",
    "            # Complex business logic\n",
    "            if item[\"subscription\"] == \"premium\":\n",
    "                if item[\"region\"] == \"US\":\n",
    "                    processed[\"discount\"] = 0.1\n",
    "                    if item[\"loyalty_years\"] > 5:\n",
    "                        processed[\"discount\"] = 0.15\n",
    "                        if item[\"referrals\"] > 10:\n",
    "                            processed[\"discount\"] = 0.2\n",
    "                elif item[\"region\"] == \"EU\":\n",
    "                    processed[\"discount\"] = 0.08\n",
    "                    if item[\"loyalty_years\"] > 3:\n",
    "                        processed[\"discount\"] = 0.12\n",
    "                else:\n",
    "                    processed[\"discount\"] = 0.05\n",
    "            else:\n",
    "                processed[\"discount\"] = 0.0\n",
    "            \n",
    "            result.append(processed)\n",
    "            print(f\"Processed user: {processed['name']}\")\n",
    "    \n",
    "    print(f\"Total processed: {len(result)}\")\n",
    "    return result\n",
    "'''\n",
    "\n",
    "print(\"üìã C√≥digo legacy de ejemplo cargado\")\n",
    "print(\"Problemas visibles:\")\n",
    "print(\"  ‚Ä¢ Funci√≥n muy larga y compleja\")\n",
    "print(\"  ‚Ä¢ L√≥gica de negocio anidada\")\n",
    "print(\"  ‚Ä¢ TODOs sin resolver\")\n",
    "print(\"  ‚Ä¢ Uso excesivo de print\")\n",
    "print(\"  ‚Ä¢ Falta de validaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES DE AN√ÅLISIS DE C√ìDIGO (faltantes)\n",
    "import ast\n",
    "import logging\n",
    "\n",
    "def analyze_code_complexity(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analizar complejidad del c√≥digo usando AST\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        \n",
    "        complexity_score = 0\n",
    "        functions = []\n",
    "        issues = []\n",
    "        \n",
    "        # Analizar nodos del AST\n",
    "        for node in ast.walk(tree):\n",
    "            # Contar estructuras de control (aumentan complejidad)\n",
    "            if isinstance(node, (ast.If, ast.For, ast.While, ast.With)):\n",
    "                complexity_score += 1\n",
    "            # Contar funciones\n",
    "            elif isinstance(node, ast.FunctionDef):\n",
    "                functions.append(node.name)\n",
    "                # Calcular complejidad de la funci√≥n\n",
    "                func_complexity = sum(1 for n in ast.walk(node) \n",
    "                                    if isinstance(n, (ast.If, ast.For, ast.While, ast.With)))\n",
    "                if func_complexity > 5:\n",
    "                    issues.append(f\"Funci√≥n '{node.name}' muy compleja ({func_complexity} puntos)\")\n",
    "        \n",
    "        # M√©tricas b√°sicas\n",
    "        lines = len([line for line in code.split('\\n') if line.strip()])\n",
    "        code_lines = len([line for line in code.split('\\n') \n",
    "                         if line.strip() and not line.strip().startswith('#')])\n",
    "        \n",
    "        # Calcular √≠ndice de mantenibilidad (escala 0-100)\n",
    "        maintainability_index = max(0, 100 - complexity_score * 2 - (code_lines / 10))\n",
    "        \n",
    "        # Detectar problemas adicionales\n",
    "        if 'TODO' in code:\n",
    "            issues.append(\"TODOs pendientes en el c√≥digo\")\n",
    "        if code.count('print(') > 3:\n",
    "            issues.append(\"Uso excesivo de print statements\")\n",
    "        if len(functions) == 0:\n",
    "            issues.append(\"No hay funciones definidas\")\n",
    "        \n",
    "        return {\n",
    "            \"complexity_score\": complexity_score,\n",
    "            \"average_complexity\": complexity_score / max(len(functions), 1),\n",
    "            \"maintainability_index\": maintainability_index,\n",
    "            \"code_lines\": code_lines,\n",
    "            \"total_lines\": lines,\n",
    "            \"functions_count\": len(functions),\n",
    "            \"complex_functions\": [f for f in functions if f in [issue.split(\"'\")[1] for issue in issues if \"muy compleja\" in issue]],\n",
    "            \"issues\": issues\n",
    "        }\n",
    "        \n",
    "    except SyntaxError as e:\n",
    "        return {\n",
    "            \"complexity_score\": 100,  # C√≥digo con errores de sintaxis = alta complejidad\n",
    "            \"average_complexity\": 100,\n",
    "            \"maintainability_index\": 0,\n",
    "            \"code_lines\": 0,\n",
    "            \"total_lines\": len(code.split('\\n')),\n",
    "            \"functions_count\": 0,\n",
    "            \"complex_functions\": [],\n",
    "            \"issues\": [f\"Error de sintaxis: {str(e)}\"]\n",
    "        }\n",
    "\n",
    "def analyze_code_patterns(code: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Analizar patrones y anti-patrones en el c√≥digo\"\"\"\n",
    "    good_patterns = []\n",
    "    anti_patterns = []\n",
    "    suggestions = []\n",
    "    \n",
    "    # Detectar patrones positivos\n",
    "    if 'def ' in code:\n",
    "        good_patterns.append(\"Funciones definidas (modularidad)\")\n",
    "    if '\"\"\"' in code or \"'''\" in code:\n",
    "        good_patterns.append(\"Documentaci√≥n con docstrings\")\n",
    "    if 'try:' in code and 'except' in code:\n",
    "        good_patterns.append(\"Manejo de errores implementado\")\n",
    "    if 'class ' in code:\n",
    "        good_patterns.append(\"Uso de clases (OOP)\")\n",
    "    if 'import ' in code:\n",
    "        good_patterns.append(\"Imports organizados\")\n",
    "    \n",
    "    # Detectar anti-patrones\n",
    "    if code.count('if ') > 5 and 'elif' not in code:\n",
    "        anti_patterns.append(\"M√∫ltiples if anidados (considerar elif)\")\n",
    "    if 'print(' in code and code.count('print(') > 2:\n",
    "        anti_patterns.append(\"Uso excesivo de print (usar logging)\")\n",
    "    if 'TODO' in code:\n",
    "        anti_patterns.append(\"TODOs sin resolver\")\n",
    "    if len([line for line in code.split('\\n') if len(line) > 100]) > 0:\n",
    "        anti_patterns.append(\"L√≠neas muy largas (>100 caracteres)\")\n",
    "    \n",
    "    # Contar niveles de indentaci√≥n\n",
    "    max_indent = max([len(line) - len(line.lstrip()) for line in code.split('\\n')] + [0])\n",
    "    if max_indent > 16:  # >4 niveles de indentaci√≥n\n",
    "        anti_patterns.append(f\"Anidamiento profundo ({max_indent//4} niveles)\")\n",
    "    \n",
    "    # Generar sugerencias\n",
    "    if 'TODO' in code:\n",
    "        suggestions.append(\"Resolver TODOs pendientes\")\n",
    "    if anti_patterns:\n",
    "        suggestions.append(\"Refactorizar para reducir complejidad\")\n",
    "    if 'print(' in code:\n",
    "        suggestions.append(\"Reemplazar prints con logging estructurado\")\n",
    "    if max_indent > 12:\n",
    "        suggestions.append(\"Extraer funciones para reducir anidamiento\")\n",
    "    if code.count('def ') == 0:\n",
    "        suggestions.append(\"Dividir c√≥digo en funciones reutilizables\")\n",
    "    \n",
    "    return {\n",
    "        \"good_patterns\": good_patterns,\n",
    "        \"anti_patterns\": anti_patterns,\n",
    "        \"suggestions\": suggestions,\n",
    "        \"complexity_indicators\": {\n",
    "            \"max_indentation\": max_indent,\n",
    "            \"function_count\": code.count('def '),\n",
    "            \"print_statements\": code.count('print('),\n",
    "            \"todo_count\": code.count('TODO')\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funciones de an√°lisis de c√≥digo agregadas:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AN√ÅLISIS AUTOM√ÅTICO DEL C√ìDIGO LEGACY\n",
    "import asyncio\n",
    "\n",
    "async def analyze_legacy_code():\n",
    "    \"\"\"Ejecutar an√°lisis completo del c√≥digo legacy\"\"\"\n",
    "    print(\"üîç Iniciando an√°lisis autom√°tico del c√≥digo legacy...\")\n",
    "    \n",
    "    # 1. An√°lisis de complejidad\n",
    "    complexity_analysis = analyze_code_complexity(sample_legacy_code)\n",
    "    print(f\"\\nüìä AN√ÅLISIS DE COMPLEJIDAD:\")\n",
    "    print(f\"  ‚Ä¢ Complejidad m√°xima: {complexity_analysis.get('complexity_score', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ Complejidad promedio: {complexity_analysis.get('average_complexity', 'N/A'):.1f}\")\n",
    "    print(f\"  ‚Ä¢ √çndice mantenibilidad: {complexity_analysis.get('maintainability_index', 'N/A'):.1f}\")\n",
    "    print(f\"  ‚Ä¢ L√≠neas de c√≥digo: {complexity_analysis.get('code_lines', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ Funciones complejas: {complexity_analysis.get('complex_functions', [])}\")\n",
    "    \n",
    "    if complexity_analysis.get('issues'):\n",
    "        print(f\"  ‚ö†Ô∏è  PROBLEMAS DETECTADOS:\")\n",
    "        for issue in complexity_analysis['issues']:\n",
    "            print(f\"      - {issue}\")\n",
    "    \n",
    "    # 2. An√°lisis de patrones\n",
    "    pattern_analysis = analyze_code_patterns(sample_legacy_code)\n",
    "    print(f\"\\nüîç AN√ÅLISIS DE PATRONES:\")\n",
    "    \n",
    "    if pattern_analysis['good_patterns']:\n",
    "        print(f\"  ‚úÖ Patrones positivos:\")\n",
    "        for pattern in pattern_analysis['good_patterns']:\n",
    "            print(f\"      - {pattern}\")\n",
    "    \n",
    "    if pattern_analysis['anti_patterns']:\n",
    "        print(f\"  ‚ùå Anti-patrones detectados:\")\n",
    "        for anti_pattern in pattern_analysis['anti_patterns']:\n",
    "            print(f\"      - {anti_pattern}\")\n",
    "    \n",
    "    if pattern_analysis['suggestions']:\n",
    "        print(f\"  üí° Sugerencias de mejora:\")\n",
    "        for suggestion in pattern_analysis['suggestions']:\n",
    "            print(f\"      - {suggestion}\")\n",
    "    \n",
    "    # 3. Preparar datos para pipeline\n",
    "    analysis_data = {\n",
    "        \"code\": sample_legacy_code,\n",
    "        \"metrics\": complexity_analysis,\n",
    "        \"patterns\": pattern_analysis\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ An√°lisis completo - Datos preparados para refactoring\")\n",
    "    return analysis_data\n",
    "\n",
    "# Ejecutar an√°lisis\n",
    "legacy_analysis = await analyze_legacy_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUCI√ìN DEL REFACTORING INTELIGENTE\n",
    "async def execute_intelligent_refactoring():\n",
    "    \"\"\"Ejecutar el pipeline completo de refactoring\"\"\"\n",
    "    print(\"üöÄ Ejecutando pipeline de refactoring inteligente...\")\n",
    "    \n",
    "\n",
    "    # Ejecutar pipeline con an√°lisis previo\n",
    "    refactoring_result = await refactoring_system.ainvoke(legacy_analysis)\n",
    "    \n",
    "    # Mostrar an√°lisis t√©cnico\n",
    "    analysis = refactoring_result['original']['analysis']\n",
    "    print(f\"\\nüìã AN√ÅLISIS T√âCNICO:\")\n",
    "    print(f\"  ‚Ä¢ Prioridad de refactoring: {analysis.refactoring_priority}\")\n",
    "    print(f\"  ‚Ä¢ Esfuerzo estimado: {analysis.estimated_effort}\")\n",
    "    print(f\"  ‚Ä¢ Puntuaci√≥n mantenibilidad: {analysis.maintainability_score}/10\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  PROBLEMAS IDENTIFICADOS:\")\n",
    "    for issue in analysis.code_issues:\n",
    "        print(f\"    - {issue}\")\n",
    "    \n",
    "    print(f\"\\nüí° SUGERENCIAS DE MEJORA:\")\n",
    "    for suggestion in analysis.improvement_suggestions:\n",
    "        print(f\"    - {suggestion}\")\n",
    "    \n",
    "    # Mostrar c√≥digo refactorizado\n",
    "    refactored = refactoring_result['refactored']\n",
    "    print(f\"\\n‚ú® C√ìDIGO REFACTORIZADO:\")\n",
    "    print(\"=\"*60)\n",
    "    print(refactored.improved_code)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìù RESUMEN DE CAMBIOS:\")\n",
    "    for change in refactored.changes_summary:\n",
    "        print(f\"    ‚úì {change}\")\n",
    "    \n",
    "    if refactored.performance_improvements:\n",
    "        print(f\"\\n‚ö° MEJORAS DE RENDIMIENTO:\")\n",
    "        for improvement in refactored.performance_improvements:\n",
    "            print(f\"    ‚ö° {improvement}\")\n",
    "    \n",
    "    if refactored.code_style_fixes:\n",
    "        print(f\"\\nüé® CORRECCIONES DE ESTILO:\")\n",
    "        for fix in refactored.code_style_fixes:\n",
    "            print(f\"    üé® {fix}\")\n",
    "    \n",
    "    return refactoring_result\n",
    "    \n",
    "\n",
    "# Ejecutar refactoring\n",
    "print(\"Iniciando proceso de refactoring...\")\n",
    "refactoring_results = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "refactoring_results = await execute_intelligent_refactoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
